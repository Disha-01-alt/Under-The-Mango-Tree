{
  "course_title": "Deep Learning & AI",
  "topics": [
    {
      "name": "Artificial Neural Networks (ANN)",
      "videos": [
        { "id": 0, "title": "ANN Basics: Introduction", "youtube_url": "https://youtu.be/6nkylSKqaAc", "what_you_will_learn": ["The structure of a neuron and its biological inspiration.", "How layers are connected in a neural network.", "The concept of Universal Approximation."], "content_covered": "This is the foundational video for Deep Learning, introducing the Artificial Neural Network (ANN). We explore its mathematical formulation and set the stage for more complex architectures.", "resources": [ { "name": "Universal Approximation Theorem", "url": "http://neuralnetworksanddeeplearning.com/chap4.html", "icon": "fas fa-book" }, { "name": "Difference between Brain and ANN", "url": "https://towardsdatascience.com/the-differences-between-artificial-and-biological-neural-networks-a8b46db828b7", "icon": "fas fa-brain" } ] },
        { "id": 1, "title": "ANN Basics: Backpropagation", "youtube_url": "https://youtu.be/ntnwjWEpnkk", "what_you_will_learn": ["The intuition behind how networks learn from error.", "Understanding the chain rule in calculus.", "How weights and biases are updated during training."], "content_covered": "Backpropagation is the algorithm that allows neural networks to learn. This video demystifies the process, explaining how the network's error is 'propagated' backward to adjust its internal parameters.", "resources": [ { "name": "Backprop from Scratch (Code)", "url": "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/", "icon": "fas fa-code" } ] },
        { "id": 2, "title": "ANN Basics: Activation & Loss", "youtube_url": "https://youtu.be/qctUEQn9Hj8", "what_you_will_learn": ["The role of activation functions like ReLU.", "What a loss function is and how it measures error.", "Overview of optimization algorithms."], "content_covered": "Dive into the key components that make training possible: activation functions that introduce non-linearity, and loss functions that guide the learning process.", "resources": [ { "name": "Activation Functions Compared", "url": "https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a", "icon": "fas fa-book-open" }, { "name": "Loss Functions Explained", "url": "https://towardsdatascience.com/understanding-different-loss-functions-for-neural-networks-dd1ed0274718", "icon": "fas fa-book-open" }, { "name": "Optimization Algorithms Paper", "url": "https://arxiv.org/pdf/1609.04747", "icon": "fas fa-file-pdf" } ] },
        { "id": 3, "title": "Advanced ANN: Regularization & Dropout", "youtube_url": "https://www.youtube.com/watch?v=dXB-KQYkzNU", "what_you_will_learn": ["How to prevent model overfitting.", "Understanding L1/L2 Regularization.", "How Dropout works as a regularization technique."], "content_covered": "Overfitting is a major challenge in deep learning. This video covers key regularization techniques designed to make your models generalize better to new, unseen data, with a focus on the popular Dropout method.", "resources": [ { "name": "Regularization for ANN", "url": "https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3", "icon": "fas fa-book-open" }, { "name": "Dropout in Neural Networks", "url": "https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9", "icon": "fas fa-book-open" }, { "name": "Dropout Video Explainer", "url": "https://www.youtube.com/watch?v=qfsacble9Al", "icon": "fab fa-youtube" } ] },
        { "id": 4, "title": "Advanced ANN: Practical Techniques", "youtube_url": "https://www.youtube.com/watch?v=Gey9CG6R6w8", "what_you_will_learn": ["Understanding the Vanishing Gradient Problem.", "Using PyTorch for building networks.", "Using TensorFlow for building networks."], "content_covered": "This video covers several practical aspects of deep learning, including the infamous vanishing gradient problem and introductions to the two most popular DL frameworks: PyTorch and TensorFlow.", "resources": [ { "name": "Vanishing Gradient Problem", "url": "https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484", "icon": "fas fa-book-open" }, { "name": "PyTorch Introduction Video", "url": "https://www.youtube.com/watch?v=Uv0AIRr3ptg", "icon": "fab fa-youtube" }, { "name": "PyTorch Colab Notebook", "url": "https://colab.research.google.com/drive/1Pz8b_h-W9zlBk1p2e6v-YFYThG1NkYeS?usp=sharing", "icon": "fab fa-google-drive" }, { "name": "TensorFlow Playlist", "url": "https://www.youtube.com/watch?v=OHZqmJwi7n4&list=PL9ooVrP1hQOFJ8UZI86fYfmB1_P5yGzBT", "icon": "fab fa-youtube" } ] },
        { "id": 5, "title": "Deep Learning Intro", "youtube_url": "https://youtu.be/MTbBOu4M7_M", "what_you_will_learn": ["A high-level overview of the Deep Learning field.", "The relationship between Deep Learning, Machine Learning, and AI.", "A look at the impact and applications of modern deep learning."], "content_covered": "This video serves as a broad introduction to the entire field of Deep Learning, explaining its significance and showcasing some of the incredible applications it powers today.", "resources": [ { "name": "House Price Prediction (ANN Example)", "url": "https://medium.com/@robertjohn_15390/simple-housing-price-prediction-using-neural-networks-with-tensorflow-8b486d3db3ca", "icon": "fas fa-home" }, { "name": "Image Recognition with Keras", "url": "https://nextjournal.com/gkoehler/digit-recognition-with-keras", "icon": "fas fa-image" } ] }
      ]
    },
    {
      "name": "Boosting Algorithms",
      "videos": [
        { "id": 6, "title": "Boosting Explained", "youtube_url": "https://www.youtube.com/watch?v=kho6oANGu_A", "what_you_will_learn": ["The core idea of ensemble learning via boosting.", "How models are built sequentially to correct errors.", "Introduction to AdaBoost and Gradient Boosting."], "content_covered": "Boosting is a powerful ensemble technique that combines multiple weak learners into a single strong learner. This video explains the sequential nature of boosting, where each new model focuses on the mistakes of the previous one.", "resources": [ { "name": "Gradient Boosting Neural Networks", "url": "https://www.geeksforgeeks.org/grownet-gradient-boosting-neural-networks/", "icon": "fas fa-link" } ] },
        { "id": 7, "title": "Boosting In-Depth Part 1", "youtube_url": "https://www.youtube.com/watch?v=LsK-xG1cLYA", "what_you_will_learn": ["A deeper mathematical look at boosting algorithms.", "The role of residuals in Gradient Boosting.", "Step-by-step walkthrough of the algorithm."], "content_covered": "This video provides a more detailed, mathematical exploration of how boosting algorithms like Gradient Boosting work under the hood.", "resources": [] },
        { "id": 8, "title": "Boosting In-Depth Part 2", "youtube_url": "https://www.youtube.com/watch?v=OtD8wVaFm6E", "what_you_will_learn": ["Practical considerations for implementing boosting.", "Hyperparameter tuning for boosting models.", "Comparison of different boosting libraries like XGBoost and LightGBM."], "content_covered": "A continuation of the deep dive into boosting, this video focuses on practical implementation details and how to tune these powerful models for best performance.", "resources": [ { "name": "Boltzmann Machines (Related Concept)", "url": "#", "icon": "fas fa-project-diagram" }] }
      ]
    },
    {
      "name": "Computer Vision - CNNs",
      "videos": [
        { "id": 9, "title": "Intro to CNNs", "youtube_url": "https://www.youtube.com/watch?v=UHBmv7qCey4", "what_you_will_learn": ["Why regular ANNs aren't ideal for images.", "The concept of kernels and feature maps.", "How convolution and pooling layers work."], "content_covered": "Convolutional Neural Networks (CNNs) revolutionized computer vision. This video explains the specialized layers that allow CNNs to efficiently process image data by learning spatial hierarchies of features.", "resources": [ { "name": "A Guide to CNNs", "url": "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53", "icon": "fas fa-book-open" }, { "name": "NPTEL DL for CV Course", "url": "https://dl4cv-nptel.github.io/DL4CVBK/intro.html", "icon": "fas fa-graduation-cap" } ] },
        { "id": 10, "title": "Backpropagation in CNNs", "youtube_url": "https://www.youtube.com/watch?v=pUCCd2-17vl", "what_you_will_learn": ["How backpropagation is adapted for convolutional layers.", "How filter weights are updated.", "The flow of gradients through the network."], "content_covered": "This video demystifies how a CNN learns by explaining the process of backpropagation as it applies to the unique architecture of convolutional and pooling layers.", "resources": [] },
        { "id": 11, "title": "CNN Architectures", "youtube_url": "https://www.youtube.com/watch?v=DAOcjicFr1Y", "what_you_will_learn": ["The evolution of famous architectures like LeNet, AlexNet, VGG, ResNet.", "Key innovations like residual connections.", "Trade-offs between model complexity and performance."], "content_covered": "This video takes a tour through the history of landmark CNN architectures, highlighting the key breakthroughs that have pushed the boundaries of computer vision.", "resources": [ { "name": "CNN Architectures Overview", "url": "https://www.geeksforgeeks.org/convolutional-neural-network-cnn-architectures/", "icon": "fas fa-link" }, { "name": "History of CNNs (AlexNet to NASNet)", "url": "https://towardsdatascience.com/from-alexnet-to-nasnet-a-brief-history-and-introduction-of-convolutional-neural-networks-cf63bf3320e1", "icon": "fas fa-book-open" } ] }
      ]
    },
    {
      "name": "Advanced Vision Models",
      "videos": [
        { "id": 12, "title": "Autoencoders", "youtube_url": "https://www.youtube.com/watch?v=5W0ltGTWV54", "what_you_will_learn": ["The encoder-decoder structure of an autoencoder.", "Applications in dimensionality reduction and anomaly detection.", "The concept of a compressed 'latent space' representation."], "content_covered": "Autoencoders are an unsupervised neural network that learns efficient data codings. This video explains their architecture and common use cases.", "resources": [] },
        { "id": 13, "title": "Generative Adversarial Networks (GANs)", "youtube_url": "https://www.youtube.com/watch?v=9zKuYvjFFS8", "what_you_will_learn": ["The concept of generative models.", "The adversarial training process of a Generator and a Discriminator.", "Applications in generating realistic synthetic data."], "content_covered": "GANs consist of two competing neural networks, a generator and a discriminator, which are trained together to create highly realistic, synthetic data like images or text.", "resources": [ { "name": "GANs for MNIST Digits (Code)", "url": "https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/", "icon": "fas fa-code" }, { "name": "GAN Wiki", "url": "https://wiki.pathmind.com/generative-adversarial-network-gan", "icon": "fas fa-book" } ] }
      ]
    },
    {
      "name": "Sequence Modeling (NLP)",
      "videos": [
        { "id": 14, "title": "RNN & LSTM Theory", "youtube_url": "https://www.youtube.com/watch?v=6niqTuYFZLQ", "what_you_will_learn": ["How Recurrent Neural Networks (RNNs) process sequential data.", "The vanishing gradient problem in RNNs.", "How Long Short-Term Memory (LSTM) networks solve this with gates."], "content_covered": "This video introduces models for sequential data like text or time series. It covers RNN basics and explains why LSTMs are often preferred for their ability to remember long-term dependencies.", "resources": [ { "name": "Understanding LSTMs (Colah's Blog)", "url": "http://colah.github.io/posts/2015-08-Understanding-LSTMS/", "icon": "fas fa-book-open" }, { "name": "The Unreasonable Effectiveness of RNNs", "url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "icon": "fas fa-link" }, { "name": "Word Embeddings Survey", "url": "https://medium.com/@phylypo/a-survey-of-the-state-of-the-art-language-models-up-to-early-2020-aba824302c6", "icon": "fas fa-book" } ] },
        { "id": 15, "title": "The Transformer Architecture", "youtube_url": "https://www.youtube.com/watch?v=SZorAJ4I-sA", "what_you_will_learn": ["The limitation of sequential processing in RNNs.", "The revolutionary Self-Attention mechanism.", "How Transformers process sequences in parallel."], "content_covered": "The Transformer architecture is the backbone of modern Large Language Models (LLMs). This video breaks down the Self-Attention mechanism that allows it to understand complex context in text.", "resources": [ { "name": "The Illustrated Transformer", "url": "https://jalammar.github.io/illustrated-transformer/", "icon": "fas fa-image" }, { "name": "Annotated Transformer (Code)", "url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html", "icon": "fas fa-code" } ] },
        { "id": 16, "title": "LLM Pre-Training Concepts", "youtube_url": "https://www.youtube.com/watch?v=knTc-NQSjKA", "what_you_will_learn": ["What it means to 'pre-train' a large model.", "Common objectives like Masked Language Modeling (MLM).", "The scale of data and computation required."], "content_covered": "Learn about the crucial first step in creating powerful LLMs: pre-training on vast amounts of text data to learn general language understanding.", "resources": [ { "name": "LLM from Scratch Notebooks", "url": "https://github.com/anishiisc/Build_LLM_from_Scratch/tree/main", "icon": "fab fa-github" } ] },
        { "id": 17, "title": "Fine-Tuning LLMs", "youtube_url": "https://www.youtube.com/watch?v=mw7ay38--ak", "what_you_will_learn": ["What fine-tuning is and why it's necessary.", "How to adapt a pre-trained model for a specific task.", "The concept of transfer learning in NLP."], "content_covered": "Fine-tuning is the process of taking a general-purpose pre-trained model and specializing it for a particular task like sentiment analysis or question answering. This video explains the methodology.", "resources": [ { "name": "Fine-tuning BERT Code", "url": "https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python", "icon": "fas fa-code" }, { "name": "Named Entity Recognition with BERT", "url": "https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/", "icon": "fas fa-book-open" }, { "name": "Fine-Tuning Video 2", "url": "https://www.youtube.com/watch?v=kCc8FmEb1nY", "icon": "fab fa-youtube" } ] }
      ]
    },
    {
      "name": "Advanced NLP Applications",
      "videos": [
        { "id": 18, "title": "Retrieval Augmented Generation (RAG)", "youtube_url": "https://www.youtube.com/watch?v=wd7TZ4w1mSw", "what_you_will_learn": ["The limitations of LLMs (knowledge cutoffs, hallucination).", "How RAG enhances LLMs with external knowledge.", "The workflow: retrieve relevant documents, then generate an answer."], "content_covered": "RAG is a technique for making LLMs more accurate and reliable by grounding them in external, up-to-date knowledge sources. This video explains the architecture.", "resources": [ {"name": "RAG with LangChain", "url": "#", "icon": "fas fa-link"} ] },
        { "id": 19, "title": "RAG In-Depth", "youtube_url": "https://www.youtube.com/watch?v=ahnGLM-RC1Y", "what_you_will_learn": ["Vector databases and embeddings for retrieval.", "Different strategies for combining retrieved context with the prompt.", "Practical challenges in implementing RAG systems."], "content_covered": "This video goes deeper into the implementation details of a RAG system, covering the crucial components like vector databases and different strategies for prompt engineering.", "resources": [ {"name": "Sentence Similarity with BERT/SBERT", "url": "#", "icon": "fas fa-book"} ] }
      ]
    },
    {
      "name": "Multi-Modal AI",
      "videos": [
        { "id": 20, "title": "Image Captioning", "youtube_url": "https://www.youtube.com/watch?v=JmATtG0yA5E", "what_you_will_learn": ["The challenge of combining vision and language models.", "Common architectures for image captioning (e.g., Encoder-Decoder).", "How attention mechanisms help focus on relevant parts of the image."], "content_covered": "Explore the exciting field of multi-modal AI where models learn to bridge the gap between vision and language by generating descriptive text captions for images.", "resources": [] },
        { "id": 21, "title": "Text-to-Image Generation (Diffusion)", "youtube_url": "https://www.youtube.com/watch?v=pea3sH6orMc", "what_you_will_learn": ["The basics of diffusion models for text-to-image generation.", "How models like DALL-E and Stable Diffusion work.", "The concept of CLIP for guiding the generation process."], "content_covered": "This video introduces the powerful technology behind text-to-image models, explaining how diffusion models can create stunning and complex images from simple text prompts.", "resources": [ { "name": "Illustrated Stable Diffusion", "url": "https://jalammar.github.io/illustrated-stable-diffusion/", "icon": "fas fa-image" }, { "name": "Diffusion Models Video Explainer", "url": "https://www.youtube.com/watch?v=9BHQvQIsVdE", "icon": "fab fa-youtube" } ] }
      ]
    },
    {
      "name": "Deployment",
      "videos": [
        { "id": 22, "title": "Reading: Deploying ML Models", "youtube_url": "#", "what_you_will_learn": ["Considerations for taking a model from research to production.", "Common deployment patterns (e.g., API endpoints).", "Tools and platforms for model serving and MLOps."], "content_covered": "Training a model is only half the battle. This topic links to a comprehensive course that discusses the challenges and best practices for deploying machine learning models into real-world applications so they can provide value.", "resources": [ { "name": "Full Stack Deep Learning Course", "url": "https://fullstackdeeplearning.com/course/2022/", "icon": "fas fa-graduation-cap" } ] }
      ]
    }
  ]
}
